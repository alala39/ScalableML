{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes are from Fandango, Armando. Mastering TensorFlow 1.x: Advanced machine learning and deep learning concepts using TensorFlow 1.x and Keras (Kindle Locations 1020-1024). Packt Publishing. Kindle Edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFLearn MNIST Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFLearn can be installed in Python 3 with the following command: pip3 install tflearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple workflow in TFLearn is as follows: \n",
    "\n",
    "Create an input layer first. \n",
    "\n",
    "Pass the input object to create further layers. \n",
    "\n",
    "Add the output layer. \n",
    "\n",
    "Create the net using an estimator layer such as regression. \n",
    "\n",
    "Create a model from the net created in the previous step. \n",
    "\n",
    "Train the model with the model.fit() method. \n",
    "\n",
    "Use the trained model to predict or evaluate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tflearn\n",
    "import tflearn.datasets.mnist as mnist\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_classes = 10\n",
    "n_epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = mnist.load_data(\n",
    "    data_dir=os.path.join('.', 'mnist'), one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build deep neural network\n",
    "\n",
    "#Create an input layer first:\n",
    "input_layer = tflearn.input_data(shape=[None, 784])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the input object to create further layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = tflearn.fully_connected(input_layer,\n",
    "                                 10,\n",
    "                                 activation='relu'\n",
    "                                 )\n",
    "layer2 = tflearn.fully_connected(layer1,\n",
    "                                 10,\n",
    "                                 activation='relu'\n",
    "                                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the output layer\n",
    "output = tflearn.fully_connected(layer2,\n",
    "                                 n_classes,\n",
    "                                 activation='softmax'\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the final net from the estimator layer such as regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tflearn/objectives.py:66: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "net = tflearn.regression(output,\n",
    "                         optimizer='adam',\n",
    "                         metric=tflearn.metrics.Accuracy(),\n",
    "                         loss='categorical_crossentropy'\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model from the net created in the previous step.\n",
    "\n",
    "The TFLearn offers two different classes of the models: \n",
    "\n",
    "DNN (Deep Neural Network) model: This class allows you to create a multilayer perceptron from the network that you have created from the layers \n",
    "\n",
    "SequenceGenerator model: This class allows you to create a deep neural network that can generate sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tflearn.DNN(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating, train the model with the model.fit() method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2749  | total loss: \u001b[1m\u001b[32m0.34866\u001b[0m\u001b[0m | time: 2.786s\n",
      "| Adam | epoch: 005 | loss: 0.34866 - acc: 0.8970 -- iter: 54900/55000\n",
      "Training Step: 2750  | total loss: \u001b[1m\u001b[32m0.34889\u001b[0m\u001b[0m | time: 2.791s\n",
      "| Adam | epoch: 005 | loss: 0.34889 - acc: 0.8983 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    n_epoch=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    show_metric=True,\n",
    "    run_id='dense_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained model to predict or evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8996\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test accuracy:', score[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFLearn core layers\n",
    " \n",
    "\n",
    "TFLearn offers the following layers in the tflearn.layers.core module: \n",
    "\n",
    "input_data: This layer is used to specify the input layer for the neural network. \n",
    "\n",
    "fully_connected: This layer is used to specify a layer where all the neurons are connected to all the neurons in the previous layer. \n",
    "\n",
    "dropout: This layer is used to specify the dropout regularization. The input elements are scaled by 1/ keep_prob while keeping the expected sum unchanged. \n",
    "\n",
    "custom_layer: This layer is used to specify a custom function to be applied to the input. This class wraps our custom function and presents the function as a layer. \n",
    "\n",
    "reshape: This layer reshapes the input into the output of specified shape. \n",
    "\n",
    "flatten: This layer converts the input tensor to a 2D tensor. activation This layer applies the specified activation function to the input tensor. \n",
    "\n",
    "single_unit: This layer applies the linear function to the inputs. \n",
    "\n",
    "highway: This layer implements the fully connected highway function. \n",
    "\n",
    "one_hot_encoding: This layer converts the numeric labels to their binary vector one-hot encoded representations. \n",
    "\n",
    "time_distributed: This layer applies the specified function to each time step of the input tensor. \n",
    "\n",
    "multi_target_data: This layer creates and concatenates multiple placeholders, specifically used when the layers use targets from multiple sources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFLearn convolutional layers \n",
    "\n",
    "TFLearn offers the following layers in the tflearn.layers.conv module: \n",
    "\n",
    "conv_1d: This layer applies 1D convolutions to the input data \n",
    "\n",
    "conv_2d: This layer applies 2D convolutions to the input data \n",
    "\n",
    "conv_3d: This layer applies 3D convolutions to the input data \n",
    "\n",
    "conv_2d_transpose: This layer applies transpose of conv2_d to the input data \n",
    "\n",
    "conv_3d_transpose: This layer applies transpose of conv3_d to the input data \n",
    "\n",
    "atrous_conv_2d: This layer computes a 2-D atrous convolution \n",
    "\n",
    "grouped_conv_2d: This layer computes a depth-wise 2-D convolution \n",
    "\n",
    "max_pool_1d: This layer computes 1-D max pooling \n",
    "\n",
    "max_pool_2d: This layer computes 2D max pooling \n",
    "\n",
    "avg_pool_1d: This layer computes 1D average pooling \n",
    "\n",
    "avg_pool_2d: This layer computes 2D average pooling \n",
    "\n",
    "upsample_2d: This layer applies the row and column wise 2-D repeat operation \n",
    "\n",
    "upscore_layer: This layer implements the upscore as specified in http:// arxiv.org/ abs/ 1411.4038 \n",
    "\n",
    "global_max_pool: This layer implements the global max pooling operation \n",
    "\n",
    "global_avg_pool: This layer implements the global average pooling operation \n",
    "\n",
    "residual_block: This layer implements the residual block to create deep residual networks \n",
    "\n",
    "residual_bottleneck: This layer implements the residual bottleneck block for deep residual networks \n",
    "\n",
    "resnext_block: This layer implements the ResNeXt block\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFLearn recurrent layers \n",
    "\n",
    "TFLearn offers the following layers in the tflearn.layers.recurrent module: \n",
    "\n",
    "simple_rnn: This layer implements the simple recurrent neural network \n",
    "\n",
    "model bidirectional_rnn: This layer implements the bi-directional RNN model \n",
    "\n",
    "lstm: This layer implements the LSTM model \n",
    "\n",
    "gru: This layer implements the GRU model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "TFLearn offers the following optimizer functions as classes in the tflearn.optimizers module: \n",
    "\n",
    "SGD \n",
    "\n",
    "RMSprop \n",
    "\n",
    "Adam \n",
    "\n",
    "Momentum \n",
    "\n",
    "AdaGrad \n",
    "\n",
    "Ftrl \n",
    "\n",
    "AdaDelta \n",
    "\n",
    "ProximalAdaGrad \n",
    "\n",
    "Nesterov \n",
    "\n",
    "You can create custom optimizers by extending the tflearn.optimizers.Optimizer base class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFLearn documentation: http://tflearn.org/ \n",
    "\n",
    "source: https://github.com/tflearn/tflearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
